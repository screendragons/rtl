{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (/Users/hannatoenbreker/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/nl/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n",
      "Found cached dataset common_voice_11_0 (/Users/hannatoenbreker/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/nl/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 41054\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
      "        num_rows: 10743\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"nl\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"nl\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/hannatoenbreker/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare feature extractor and tokenizer \n",
    "### The ASR pipeline can be de-composed into three components:\n",
    "\n",
    "- A feature extractor which pre-processes the raw audio-inputs\n",
    "- The model which performs the sequence-to-sequence mapping\n",
    "- A tokenizer which post-processes the model outputs to text format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor\n",
    "- Eerst worden audiomonsters opgevuld/afgekapt zodat alle monsters een invoerlengte van 30s hebben.\n",
    "- De tweede bewerking die de Whisper-extractor uitvoert is het converteren van de opgevulde audio-arrays naar log-Mel spectrogrammen. Deze spectrogrammen zijn een visuele weergave van de frequenties van een signaal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper Tokenizer\n",
    "Het Whisper model voert tekst tokens uit die de index van de voorspelde tekst in de woordenlijst van vocabulaire items aangeven. De tokenizer zet een opeenvolging van teksttokens om in de werkelijke tekststring (bijv. [1169, 3797, 3332] -> \"de kat zat\"). Oftewel encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Dutch\", task=\"transcribe\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bij het coderen van de transcripties voegt de tokenizer 'speciale tokens' toe aan het begin en einde van de sequentie, inclusief het begin/eind van transcripttokens, de taaltokens en de taaktokens (zoals gespecificeerd door de argumenten in de vorige stap). Bij het decoderen van de label-id's hebben we de optie om deze speciale tokens 'over te slaan', zodat we een string in de oorspronkelijke invoer kunnen retourneren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/features/audio.py:313: UserWarning: \n",
      "To support 'mp3' decoding with `torchaudio>=0.12.0`, please install `ffmpeg4` system package. On Google Colab you can run:\n",
      "\n",
      "\t!add-apt-repository -y ppa:jonathonf/ffmpeg-4 && apt update && apt install -y ffmpeg\n",
      "\n",
      "and restart your runtime. Alternatively, you can downgrade `torchaudio`:\n",
      "\n",
      "\tpip install \"torchaudio<0.12\"`.\n",
      "\n",
      "Otherwise 'mp3' files will be decoded with `librosa`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/features/audio.py:334: UserWarning: Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\n",
      "  warnings.warn(\"Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 Wij hebben ons nauwgezet aan die wens gehouden.\n",
      "Decoded w/ special:    <|startoftranscript|><|nl|><|transcribe|><|notimestamps|>Wij hebben ons nauwgezet aan die wens gehouden.<|endoftext|>\n",
      "Decoded w/out special: Wij hebben ons nauwgezet aan die wens gehouden.\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om het gebruik van de feature extractor en tokenizer te vereenvoudigen, kunnen we beide in een enkele WhisperProcessor klasse onderbrengen. Dit processorobject erft van de WhisperFeatureExtractor en WhisperProcessor en kan gebruikt worden op de audio inputs en modelvoorspellingen zoals vereist. Op deze manier hoeven we maar twee objecten bij te houden tijdens de training: de processor en het model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Dutch\", task=\"transcribe\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/Users/hannatoenbreker/.cache/huggingface/datasets/downloads/extracted/bd6dfdacc71b2586341c3d59e48e79c8959528a3ed078349419a94a89b10e878/common_voice_nl_23373535.mp3', 'array': array([], dtype=float32), 'sampling_rate': 48000}, 'sentence': 'Wij hebben ons nauwgezet aan die wens gehouden.'}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen zien dat we een 1-dimensionale audio-ingangsarray en de bijbehorende transcriptie hebben. De bemonsteringsfrequentie van onze audio moet worden afgestemd op die van het Whisper-model (16 kHz). Aangezien onze audio input gesampled is op 48kHz, moeten we het downsamplen naar 16kHz voordat het doorgegeven wordt aan de Whisper feature extractor. We zetten de audio inputs op de juiste sampling rate met de cast_column methode van de dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/Users/hannatoenbreker/.cache/huggingface/datasets/downloads/extracted/bd6dfdacc71b2586341c3d59e48e79c8959528a3ed078349419a94a89b10e878/common_voice_nl_23373535.mp3', 'array': array([], dtype=float32), 'sampling_rate': 16000}, 'sentence': 'Wij hebben ons nauwgezet aan die wens gehouden.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dacb445ef14040959f82cd18da6303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/41054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41856e8f52b42e2b905a220330f9bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10743 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, num_proc=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Data Collator\n",
    "### We can leverage the WhisperProcessor we defined earlier to perform both the feature extractor and the tokenizer operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De input_features zijn al opgevuld tot 30s en geconverteerd naar een log-Mel spectrogram van vaste dimensie, dus alles wat we moeten doen is ze converteren naar gebatchte PyTorch tensoren. Converting a log-mel spectrogram to a PyTorch tensor allows for efficient data representation, standardization, normalization, and batch processing, enabling effective training and inference with deep learning models in speech recognition tasks. We doen dit met de .pad methode van de feature extractor met return_tensors=pt.\n",
    "\n",
    "Machine learning models often process data in batches for efficiency. To create batches, all input sequences within a batch need to have the same length. Padding ensures that all sequences in a batch have equal lengths, allowing for efficient parallelization during training and inference.\n",
    "\n",
    "We vullen de sequenties eerst op tot de maximale lengte in de batch met de .pad-methode van de tokenizer. De opgevulde tokens worden dan vervangen door -100 zodat deze tokens niet worden meegerekend bij het berekenen van het verlies. Vervolgens knippen we het begin van de transcripttoken af van het begin van de labelreeks als we deze later tijdens de training toevoegen. We kunnen gebruik maken van de WhisperProcessor die we eerder gedefinieerd hebben om zowel de feature extractor als de tokenizer operaties uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# A Python decorator for defining data classes, classes that just have fields with no additional methods.\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # Processor will be used for padding our features\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # We separate input features (audio) and labels (text) as they need different padding\n",
    "\n",
    "        # Prepare the audio inputs\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        \n",
    "        # Pad the audio inputs to ensure they all have the same length\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad the labels to the maximum length label in our batch\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # Replace padding tokens with -100 so they are ignored when calculating loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # Add the prepared labels to our batch\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        # Return the batch which now contains our input features and labels, both correctly padded\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our results with WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# defining metric for evaluation\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    # Get predictions and labels from the output of the model\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 (typically representing ignored tokens) with the pad_token_id.\n",
    "    # This is done to exclude such tokens from the Word Error Rate (WER) calculation.\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decoding the tokenized text back to readable text.\n",
    "    # `skip_special_tokens=True` will discard special tokens (like padding, start, end tokens) during decoding.\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute Word Error Rate (WER), which is a common metric for speech recognition systems.\n",
    "    # It's calculated as the distance between predicted and reference sequences divided by the total number of reference words.\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing our model\n",
    "Here, the Whisper model from OpenAI is being loaded. This model is specifically designed for speech recognition tasks. It's pretrained on a large corpus of data, and serves as a good starting point for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None #In some models, \n",
    "#you can force the decoder to start with a specific token ID. By setting this value to None, we are indicating that there is no specific token that the decoding process should be forced to start with.\n",
    "model.config.suppress_tokens = [] # Some models allow you to specify certain tokens that should be suppressed (i.e., never be generated) during the decoding process. By setting this value to an empty list,\n",
    "#we are indicating that no tokens should be suppressed in the decoding process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "This section defines the arguments to be used for training the model, including learning rate, maximum number of steps, gradient accumulation steps, and more. The Seq2SeqTrainingArguments class from the transformers library provides a standard way of encapsulating these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-dutch\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8, # Batch size per device during training. Can be adjusted according to the computational resources.\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x the per_device_train_batch_size\n",
    "    learning_rate=1e-5, # Learning rate for the optimizer, the learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "    warmup_steps=500, #   Used to prevent the model from separating during the initial phases of training.\n",
    "    max_steps=4000, # Total number of training steps to perform.\n",
    "    gradient_checkpointing=True, # Whether to use gradient checkpointing to save memory at the expense of slower backward pass, backward pass means that the model is updated based on the error gradient. \n",
    "    #the error gradient means how much the model weights should be changed to reduce the error.\n",
    "    fp16=False, # Whether to use 16-bit (mixed) precision training instead of 32-bit training, we use 16 bit because it is faster and uses less memory.\n",
    "    evaluation_strategy=\"steps\", # Evaluation strategy to adopt during training. 'steps' means the model is evaluated every 'eval_steps'. \n",
    "    per_device_eval_batch_size=8, # Batch size is the number of samples that are processed at once.\n",
    "    predict_with_generate=True, # Whether to use generate method for the predictions during evaluation, if set to False, the model will generate the predictions.\n",
    "    generation_max_length=225, # This parameter is a hyperparameter used during the fine-tuning process that sets the maximum length (in tokens) for the generated sequences by the model.\n",
    "    save_steps=1000, # Number of steps before the checkpoint is saved, a checkpoint is a snapshot of the model weights, it is used to resume training from the same point.\n",
    "    eval_steps=1000, # Number of steps before the model is evaluated.\n",
    "    logging_steps=25, # Number of steps before logging the training metrics.\n",
    "    report_to=[\"tensorboard\"], \n",
    "    load_best_model_at_end=True, # Whether to load the best model found during training at the end of training.\n",
    "    metric_for_best_model=\"wer\", # wer is the metric we want to use to compare the best models\n",
    "    greater_is_better=False, # Indicates whether a higher metric value is better. 'False' means a lower WER is better.\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seq2SeqTrainer class from the transformers library is used to initialize the trainer. It receives the training arguments, model, training and evaluation datasets, and the metrics computation function defined earlier. The Seq2SeqTrainer is specifically designed for sequence-to-sequence models (like speech recognition models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share our results\n",
    "### If we would like to share our training results on the hub\n",
    "#### Keep in mind that we have to change the argument \"push to hub\" two blocks above this code to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: nl, split: test\",\n",
    "    \"language\": \"nl\",\n",
    "    \"model_name\": \"Whisper Dutch - RTL\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is loading the fine-tuned model and the corresponding processor from the Hugging Face Model Hub. The WhisperProcessor handles the specific data processing steps required by the Whisper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"hannatoenbreker/whisper-dutch\")\n",
    "processor = WhisperProcessor.from_pretrained(\"hannatoenbreker/whisper-dutch\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcribe function is then defined to use this pipeline to transcribe audio input into text. A Gradio interface is created and launched. Gradio is a python library for quickly creating user interfaces to prototype machine learning models. It's using a Microphone input interface that lets users record audio directly from their microphones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"hanna/whisper-dutch\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe, \n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Dutch\",\n",
    "    description=\"Realtime demo for Dutch speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
